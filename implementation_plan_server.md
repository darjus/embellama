# Implementation Plan: Embellama Server

This document outlines the phased implementation plan for the `embellama` server component. The server provides an OpenAI-compatible REST API for the core library functionality.

## Critical Architecture Constraint

**⚠️ IMPORTANT**: Due to `LlamaContext` being `!Send`:
- Models cannot be shared between threads using `Arc`
- Each worker thread must own its model instance
- All communication uses message passing via channels
- Worker pool pattern is mandatory for concurrent requests

## Phase 1: Server Foundation
**Priority: CRITICAL** | **Estimated Time: 3-4 hours**

### Objectives
Set up the basic server infrastructure with Axum and establish the project structure.

### Tasks
- [x] Configure server dependencies in `Cargo.toml`
  - [x] Add server feature flag configuration
  - [x] Add `axum = { version = "0.8", optional = true }`
  - [x] Add `tokio = { version = "1.35", features = ["full"], optional = true }` 
  - [x] Add `clap = { version = "4.4", features = ["derive"], optional = true }`
  - [x] Add `tower = { version = "0.4", optional = true }` > NOTE: Updated to 0.5 for compatibility
  - [x] Add `tower-http = { version = "0.6", features = ["cors", "trace"], optional = true }`
  - [x] Add `uuid = { version = "1.11", features = ["v4", "serde"], optional = true }`
  - [x] Configure binary target with required features
  
- [x] Create server module structure
  - [x] Create `src/server/mod.rs` with submodule declarations
  - [x] Create `src/server/worker.rs` for worker thread implementation (stub for Phase 2)
  - [x] Create `src/server/dispatcher.rs` for request routing (stub for Phase 2)
  - [x] Create `src/server/channel.rs` for message types
  - [x] Create `src/server/state.rs` for application state
  - [x] Create `src/bin/server.rs` for server binary
  
- [x] Implement CLI argument parsing (`src/bin/server.rs`)
  - [x] Define `Args` struct with `clap` derive
  - [x] Add arguments:
    - [x] `--model-path` - Path to GGUF model file
    - [x] `--model-name` - Model identifier for API
    - [x] `--host` - Bind address (default: 127.0.0.1)
    - [x] `--port` - Server port (default: 8080)
    - [x] `--workers` - Number of worker threads
    - [x] `--queue-size` - Max pending requests per worker
    - [x] `--log-level` - Log level configuration
  - [x] Implement argument validation
  - [x] Support environment variables for all arguments
  
- [x] Set up basic Axum application
  - [x] Create `Router` with basic routes
  - [x] Add `/health` endpoint
  - [x] Configure middleware (CORS, tracing)
  - [x] Set up graceful shutdown
  - [x] Implement server binding and listening
  > BUG: Fixed layer ordering issue - TraceLayer must come before CorsLayer
  
- [x] Configure logging for server
  - [x] Set up `tracing_subscriber` with env filter
  - [x] Add request/response logging middleware
  - [x] Configure structured logging output
  - [x] Add correlation IDs for requests (using UUID v4)

### Success Criteria
- [x] Server starts with `cargo run --features server --bin embellama-server`
- [x] `/health` endpoint returns 200 OK
- [x] CLI arguments are parsed correctly
- [x] Logging produces structured output

### Dependencies
- Core library phases 1-3 (available for import)

---

## Phase 2: Worker Pool Architecture
**Priority: CRITICAL** | **Estimated Time: 6-8 hours**

### Objectives
Implement the worker pool pattern to handle the `!Send` constraint with thread-local models.

### Tasks
- [x] Define message types (`src/server/channel.rs`)
  - [x] Create `WorkerRequest` struct:
    - [x] `id: Uuid` - Request identifier
    - [x] `model: String` - Model name
    - [x] `input: TextInput` - Text(s) to process
    - [x] `response_tx: oneshot::Sender<WorkerResponse>`
  - [x] Create `WorkerResponse` struct:
    - [x] `embeddings: Vec<Vec<f32>>`
    - [x] `token_count: usize`
    - [x] `processing_time_ms: u64`
  - [x] Create `TextInput` enum for single/batch
  
- [x] Implement worker thread (`src/server/worker.rs`)
  - [x] Create `Worker` struct with:
    - [x] Shared `EmbeddingEngine` instance (thread-safe via Arc<Mutex>)
    - [x] Request receiver channel
    - [x] Worker ID and metrics
  - [x] Implement worker main loop:
    - [x] Receive requests from channel
    - [x] Process with shared engine (thread-local models managed internally)
    - [x] Send response via oneshot channel
    - [x] Handle errors gracefully
  - [x] Add worker lifecycle management:
    - [x] Initialization with engine reference
    - [x] Graceful shutdown handling
    - [x] Error recovery mechanisms
  > NOTE: Using EmbeddingEngine singleton pattern instead of direct model management
  
- [x] Implement dispatcher (`src/server/dispatcher.rs`)
  - [x] Create `Dispatcher` struct with:
    - [x] Vector of worker sender channels
    - [x] Round-robin routing via AtomicUsize
    - [x] Request queue management
  - [x] Implement request routing:
    - [x] Select next worker (round-robin)
    - [x] Forward request to worker
    - [x] Handle backpressure
    - [ ] Implement timeout handling (deferred to Phase 4)
  
- [x] Create worker pool management
  - [x] Spawn worker threads on startup
  - [x] Each worker gets engine reference (models loaded on first use)
  - [ ] Monitor worker health (deferred to Phase 4)
  - [ ] Handle worker failures/restarts (deferred to Phase 4)
  - [x] Implement graceful shutdown
  
- [x] Implement `AppState` (`src/server/state.rs`)
  - [x] Store dispatcher instance
  - [x] Store engine instance
  - [x] Configuration parameters
  - [ ] Metrics and statistics (deferred to Phase 4)
  - [x] Health check status
  
- [x] Add worker pool tests
  - [x] Test basic server startup
  - [x] Test health endpoint
  - [ ] Test concurrent requests (needs Phase 3 endpoints)
  - [ ] Test worker failure recovery (deferred to Phase 4)
  - [x] Verify no compilation errors

### Success Criteria
- [x] Worker pool starts successfully
- [x] Engine singleton initialized properly
- [x] Workers spawn and wait for requests
- [x] No data races or deadlocks
- [x] Health endpoint confirms readiness

### Dependencies
- Phase 1 (Server Foundation)

---

## Phase 3: OpenAI-Compatible API Endpoints
**Priority: HIGH** | **Estimated Time: 4-6 hours**

### Objectives
Implement the OpenAI-compatible `/v1/embeddings` endpoint with proper request/response handling.

### Tasks
- [x] Define API types (create `src/server/api_types.rs`)
  - [x] Implement `EmbeddingsRequest`:
    ```rust
    #[derive(Deserialize)]
    struct EmbeddingsRequest {
        model: String,
        input: InputType, // String or Vec<String>
        encoding_format: Option<String>, // "float" or "base64"
        dimensions: Option<usize>,
        user: Option<String>,
    }
    ```
  - [x] Implement `EmbeddingsResponse`:
    ```rust
    #[derive(Serialize)]
    struct EmbeddingsResponse {
        object: String, // "list"
        data: Vec<EmbeddingData>,
        model: String,
        usage: Usage,
    }
    ```
  - [x] Add supporting types (EmbeddingData, Usage)
  
- [x] Implement embeddings handler
  - [x] Create async handler function
  - [x] Parse and validate request
  - [x] Create oneshot channel for response
  - [x] Send request to dispatcher
  - [x] Await response with timeout
  - [x] Format OpenAI-compatible response
  
- [x] Add input validation
  - [x] Validate model name exists
  - [x] Check input text length limits
  - [x] Validate encoding format
  - [x] Handle empty inputs gracefully
  
- [x] Implement error responses
  - [x] OpenAI-compatible error format
  - [x] Appropriate HTTP status codes
  - [x] Helpful error messages
  - [x] Request ID in errors
  
- [x] Add routes to router
  - [x] Mount `/v1/embeddings` POST endpoint
  - [x] Add `/v1/models` GET endpoint
  - [ ] Add OpenAPI/Swagger documentation (deferred to Phase 6)
  
- [x] Implement content negotiation
  - [x] Support JSON requests/responses
  - [x] Handle content-type headers
  - [x] Support gzip compression (via tower-http)

### Success Criteria
- [x] Endpoint accepts OpenAI-format requests
- [x] Responses match OpenAI structure
- [x] Error handling follows OpenAI patterns
- [x] Works with OpenAI client libraries (ready for testing)

### Dependencies
- Phase 2 (Worker Pool Architecture)

---

## Phase 4: Request/Response Pipeline
**Priority: HIGH** | **Estimated Time: 5-7 hours** | **STATUS: PARTIALLY COMPLETE**

### Objectives
Implement the complete request processing pipeline with proper error handling and monitoring.

### Tasks
- [x] Implement request preprocessing
  - [x] Add request ID generation (via inject_request_id middleware)
  - [x] Implement rate limiting (token bucket with governor crate)
  - [x] Add request size limits (10MB default via limit_request_size)
  - [x] Validate authentication (API key auth with constant-time comparison)
  > NOTE: Fixed critical timing attack vulnerability using subtle crate
  
- [ ] Enhance request flow
  - [ ] Add request queuing with priorities
  - [ ] Implement request cancellation
  - [ ] Add request deduplication
  - [ ] Handle batch request optimization
  
- [ ] Implement response post-processing
  - [ ] Format embeddings (float vs base64)
  - [ ] Calculate token usage statistics
  - [ ] Add response caching headers
  - [ ] Implement response compression
  
- [x] Add timeout handling (PARTIAL)
  - [ ] Configure per-request timeouts
  - [ ] Implement timeout response
  - [ ] Clean up timed-out requests
  - [ ] Return partial results option
  > NOTE: Basic timeout support exists in handler, full implementation deferred
  
- [x] Implement backpressure handling
  - [x] Monitor queue depths (via metrics)
  - [x] Return 503 when overloaded (via circuit breaker)
  - [x] Add retry-after headers (in rate limiter responses)
  - [x] Implement circuit breaker pattern
  > NOTE: Fixed critical race conditions in circuit breaker using single mutex
  
- [x] Add observability
  - [x] Request/response metrics (Prometheus)
  - [x] Latency histograms (with configurable buckets)
  - [x] Queue depth monitoring (via gauges)
  - [x] Worker utilization metrics
  > NOTE: Comprehensive Prometheus metrics at /metrics endpoint
  
- [x] Error recovery mechanisms (PARTIAL)
  - [ ] Retry failed requests
  - [ ] Dead letter queue for failures
  - [x] Graceful degradation (via circuit breaker and load shedding)
  - [x] Health check integration (circuit breaker state in health status)

### Success Criteria
- [x] Requests process end-to-end
- [ ] Timeouts work correctly (partial - needs full implementation)
- [x] Backpressure prevents overload
- [x] Metrics are collected accurately
- [x] Error recovery functions properly (partial - basic mechanisms in place)

### Dependencies
- Phase 3 (API Endpoints)

---

## Phase 5: Integration Testing & Validation
**Priority: MEDIUM** | **Estimated Time: 6-8 hours**

### Objectives
Ensure server functionality with comprehensive integration tests and OpenAI compatibility validation.

### Tasks
- [ ] Set up test infrastructure
  - [ ] Create test server helper functions
  - [ ] Download test models for CI
  - [ ] Configure test environment
  - [ ] Set up test fixtures
  
- [ ] Write API integration tests
  - [ ] Test single embedding requests
  - [ ] Test batch embedding requests
  - [ ] Test error scenarios
  - [ ] Test timeout handling
  - [ ] Test concurrent requests
  
- [ ] OpenAI compatibility tests
  - [ ] Test with OpenAI Python client
  - [ ] Test with OpenAI JavaScript client
  - [ ] Validate response format exactly
  - [ ] Test streaming if applicable
  
- [ ] Load testing
  - [ ] Use `oha` or `wrk` for load testing
  - [ ] Test various concurrency levels
  - [ ] Measure latency percentiles
  - [ ] Find breaking points
  - [ ] Test sustained load
  
- [ ] Worker pool tests
  - [ ] Test worker failures
  - [ ] Test model reloading
  - [ ] Test graceful shutdown
  - [ ] Test resource cleanup
  
- [ ] End-to-end scenarios
  - [ ] RAG pipeline simulation
  - [ ] Semantic search workflow
  - [ ] High-volume batch processing
  - [ ] Mixed workload patterns
  
- [ ] Performance benchmarks
  - [ ] Single request latency
  - [ ] Throughput at various batch sizes
  - [ ] Memory usage under load
  - [ ] CPU utilization patterns

### Success Criteria
- [ ] All integration tests pass
- [ ] OpenAI clients work seamlessly
- [ ] Performance meets targets
- [ ] No memory leaks under load
- [ ] Graceful degradation verified

### Dependencies
- Phase 4 (Request/Response Pipeline)

---

## Phase 6: Production Features
**Priority: LOW** | **Estimated Time: 8-10 hours**

### Objectives
Add production-ready features for monitoring, operations, and deployment.

### Tasks
- [ ] Implement monitoring endpoints
  - [ ] Add `/metrics` Prometheus endpoint
  - [ ] Export key metrics:
    - [ ] Request rate and latency
    - [ ] Model inference time
    - [ ] Queue depths
    - [ ] Worker utilization
    - [ ] Error rates
  - [ ] Add custom business metrics
  
- [ ] Add operational endpoints
  - [ ] `/admin/reload` - Reload models
  - [ ] `/admin/workers` - Worker status
  - [ ] `/admin/config` - View configuration
  - [ ] `/admin/drain` - Graceful drain
  
- [ ] Implement configuration management
  - [ ] Support environment variables
  - [ ] Add configuration file support (YAML/TOML)
  - [ ] Hot-reload configuration
  - [ ] Validate configuration changes
  
- [ ] Add deployment features
  - [ ] Docker container support
  - [ ] Kubernetes readiness/liveness probes
  - [ ] Horizontal scaling support
  - [ ] Rolling update compatibility
  
- [ ] Enhance security
  - [ ] Add API key authentication
  - [ ] Implement rate limiting per client
  - [ ] Add request signing/verification
  - [ ] Audit logging for requests
  
- [ ] Implement caching layer
  - [ ] Cache frequent embeddings
  - [ ] LRU eviction policy
  - [ ] Cache metrics
  - [ ] Cache invalidation API
  
- [ ] Add multi-model support
  - [ ] Load multiple models
  - [ ] Route by model name
  - [ ] Model versioning
  - [ ] A/B testing support
  
- [ ] Production documentation
  - [ ] Deployment guide
  - [ ] Operations runbook
  - [ ] Monitoring setup
  - [ ] Troubleshooting guide

### Success Criteria
- [ ] Prometheus metrics exported
- [ ] Docker container runs successfully
- [ ] Configuration hot-reload works
- [ ] Multi-model routing functions
- [ ] Production documentation complete

### Dependencies
- Phase 5 (Integration Testing)

---

## Phase 7: Performance Optimization
**Priority: LOW** | **Estimated Time: 4-6 hours**

### Objectives
Optimize server performance based on profiling and real-world usage patterns.

### Tasks
- [ ] Profile server performance
  - [ ] CPU profiling with flamegraph
  - [ ] Memory profiling with heaptrack
  - [ ] Async runtime analysis
  - [ ] Channel contention analysis
  
- [ ] Optimize request routing
  - [ ] Implement smart load balancing
  - [ ] Add work-stealing queue
  - [ ] Optimize channel implementations
  - [ ] Reduce context switching
  
- [ ] Improve batching efficiency
  - [ ] Dynamic batch aggregation
  - [ ] Adaptive batch sizing
  - [ ] Optimize batch timeout
  - [ ] Coalesce small requests
  
- [ ] Optimize serialization
  - [ ] Use faster JSON library (simd-json)
  - [ ] Implement zero-copy where possible
  - [ ] Optimize base64 encoding
  - [ ] Buffer pool for responses
  
- [ ] Network optimizations
  - [ ] TCP tuning parameters
  - [ ] HTTP/2 support
  - [ ] Keep-alive optimization
  - [ ] Response streaming
  
- [ ] Worker pool tuning
  - [ ] Optimal worker count discovery
  - [ ] CPU affinity settings
  - [ ] NUMA awareness
  - [ ] Memory pooling

### Success Criteria
- [ ] 30% latency improvement
- [ ] 50% throughput increase
- [ ] Reduced memory footprint
- [ ] Better resource utilization

### Dependencies
- Phase 6 (Production Features)

---

## Implementation Notes

### Critical Threading Architecture

```rust
// Server architecture to handle !Send constraint:

// 1. Main thread: Axum server with tokio runtime
// 2. Worker threads: Each owns a LlamaContext
// 3. Communication: Channels only, no shared state

// AppState (Send + Sync + Clone)
struct AppState {
    dispatcher_tx: mpsc::Sender<WorkerRequest>,
    config: Arc<ServerConfig>,
    metrics: Arc<Metrics>,
}

// Worker (runs on dedicated thread)
struct Worker {
    id: usize,
    model: EmbeddingModel,  // !Send - stays on this thread
    receiver: mpsc::Receiver<WorkerRequest>,
}

// Request flow:
// HTTP Handler -> Dispatcher -> Worker -> Response Channel -> HTTP Response
```

### Deployment Considerations

1. **Memory Requirements**
   - Each worker needs full model in memory
   - Total RAM = `num_workers × model_size + overhead`
   - Example: 4 workers × 1.5GB model = 6GB minimum

2. **CPU Considerations**
   - Workers should not exceed physical cores
   - Leave cores for tokio runtime
   - Recommended: `workers = physical_cores - 2`

3. **Scaling Strategy**
   - Vertical: Add more workers (limited by RAM)
   - Horizontal: Multiple server instances
   - Use load balancer for horizontal scaling

### Monitoring Key Metrics

1. **Latency Metrics**
   - Request processing time (P50, P95, P99)
   - Model inference time
   - Queue wait time
   - Total end-to-end time

2. **Throughput Metrics**
   - Requests per second
   - Embeddings per second
   - Batch size distribution
   - Worker utilization

3. **Health Metrics**
   - Worker status
   - Queue depths
   - Error rates
   - Memory usage

### Testing Strategy

1. **Unit Tests**: Test individual components
2. **Integration Tests**: Test API endpoints
3. **Load Tests**: Verify performance under load
4. **Chaos Tests**: Test failure scenarios
5. **Compatibility Tests**: Verify OpenAI compatibility

## Success Metrics

### Performance Targets
- [ ] Single request: <100ms P99 latency
- [ ] Batch requests: >1000 embeddings/second
- [ ] Concurrent requests: Linear scaling with workers
- [ ] Memory: <2x model size per worker
- [ ] Startup time: <10 seconds

### Reliability Targets
- [ ] 99.9% uptime
- [ ] Graceful degradation under load
- [ ] Zero data loss
- [ ] Clean shutdown/restart
- [ ] Automatic recovery from failures

### Compatibility Targets
- [ ] 100% OpenAI API compatibility
- [ ] Works with all OpenAI client libraries
- [ ] Supports common embedding models
- [ ] Drop-in replacement capability